{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction\n",
    "\n",
    "We take the embedded data and calculate the cosine similarity between nodes. Cosine similarity is equal to normalized dot product similarity (source: https://zhang-yang.medium.com/cosine-similarity-dot-product-for-normalized-vectors-c07bdb61c9d1). If nodes are similar, but not linked yet, we predict a link between them.\n",
    "\n",
    "The links are predicted on the partial network (all nodes, but 80% of the links are randomly removed) and tested on the ground truth full network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    1],\n",
       "       [   0,    2],\n",
       "       [   0,    3],\n",
       "       ...,\n",
       "       [4027, 4032],\n",
       "       [4027, 4038],\n",
       "       [4031, 4038]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open link csv\n",
    "all_links = np.genfromtxt('../preprocessing/preprocessed_data/all_links.csv', delimiter=',', dtype=int)\n",
    "removed_links = np.genfromtxt('../preprocessing/preprocessed_data/removed_links.csv', delimiter=',', dtype=int)\n",
    "\n",
    "highest_node_id = all_links.max()\n",
    "all_links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_links(df):\n",
    "    # Initialize empty predicted links\n",
    "    predicted_links = []\n",
    "\n",
    "    # Iterate over all nodes and compare them to nodes they are not yet linked to\n",
    "    # Predicts link when cosine similarity > 97.5%\n",
    "    for node in range(0, highest_node_id + 1):\n",
    "        # Slices of dataframes containing the node itself\n",
    "        # and the nodes it is not connected to\n",
    "        node_df = df[df.index == node]\n",
    "        unconnected_nodes = [n for n in range(0, highest_node_id + 1) \n",
    "                             if n not in removed_links[removed_links[:,0] == node][:,1] +\n",
    "                             [node]]\n",
    "        unconnected_nodes_df = df[df.index.isin(unconnected_nodes)]\n",
    "\n",
    "        # Cosine similarity between the node itself\n",
    "        # and the nodes it is not already connected to\n",
    "        similarity_scores = cosine_similarity(node_df, unconnected_nodes_df)[0].tolist()\n",
    "        similarity_dict = dict(zip(unconnected_nodes, similarity_scores))\n",
    "            \n",
    "        # If similarity > 97.5%, append to the list of predictions\n",
    "        for pair in similarity_dict.items():\n",
    "            if pair[1] > 0.975:\n",
    "                predicted_links.append((node, pair[0]))\n",
    "\n",
    "    return predicted_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_array_to_pairs(link_array):\n",
    "    pairs = []\n",
    "\n",
    "    # Add a bidirectional pair for each link\n",
    "    # E.g. (1,0) and (0,1)\n",
    "    for link in link_array:\n",
    "        pairs.append((link[0], link[1]))\n",
    "        pairs.append((link[1], link[0]))\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pred):\n",
    "    # Convert the arrays to bidirectional lists of pairs for easier comparison\n",
    "    all_link_pairs = link_array_to_pairs(all_links)\n",
    "    removed_link_pairs = link_array_to_pairs(removed_links)\n",
    "    \n",
    "    # False positive predictions\n",
    "    # i.e. it predicted a link that isn't there\n",
    "    fp = set(pred) - set(all_link_pairs)\n",
    "\n",
    "    # False negative predictions\n",
    "    # i.e. it didnt predict a link that should have been there\n",
    "    fn = set(all_link_pairs) - set(pred) - set(removed_link_pairs)\n",
    "\n",
    "    # True positive predictions\n",
    "    # i.e. all correct predictions\n",
    "    tp = set(pred) - fp\n",
    "\n",
    "    # True negative predictions\n",
    "    # i.e. all links that were correctly not predicted\n",
    "    all_possible_pairs = itertools.product(list(range(0, highest_node_id + 1)), list(range(0, highest_node_id + 1)))\n",
    "    tn = set(all_possible_pairs) - set(all_link_pairs) - set(pred)\n",
    "    \n",
    "    total = len(tp) + len(tn) + len(fp) + len(fn)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = (len(tp) + len(tn))/total\n",
    "    \n",
    "    # Precision\n",
    "    precision = len(tp) / (len(tp) + len(fp))\n",
    "    \n",
    "    # Make confusion matrix pairs with (absolute values, percentages)\n",
    "    fp = (len(fp), len(fp)/total)\n",
    "    fn = (len(fn), len(fn)/total)\n",
    "    tp = (len(tp), len(tp)/total)\n",
    "    tn = (len(tn), len(tn)/total)\n",
    "    \n",
    "    return fp, fn, tp, tn, accuracy, precision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and make predictions\n",
    "# takes a while to run...\n",
    "node2vec = pd.read_csv('../node_embeddings/embedded_partial_data/node2vec.csv')\n",
    "node2vec_pred = predict_links(node2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node2vec\n",
      " fp:  (8828, 0.0005454621172605148) \n",
      " fn:  (31862, 0.0019686807861525285) \n",
      " tp:  (15527, 0.0009593781484712293) \n",
      " tn:  (16128225, 0.9965264789481157) \n",
      " accuracy:  0.997485857096587 \n",
      " precision:  0.6375282282898789\n"
     ]
    }
   ],
   "source": [
    "fp, fn, tp, tn, accuracy, precision = metrics(node2vec_pred)\n",
    "print('Node2vec\\n',\n",
    "      'fp: ', fp, '\\n',\n",
    "      'fn: ', fn, '\\n',\n",
    "      'tp: ', tp, '\\n',\n",
    "      'tn: ', tn, '\\n',\n",
    "      'accuracy: ', accuracy, '\\n',\n",
    "      'precision: ', precision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
