{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction\n",
    "\n",
    "We take the embedded data and calculate the cosine similarity between nodes. Cosine similarity is equal to normalized dot product similarity (source: https://zhang-yang.medium.com/cosine-similarity-dot-product-for-normalized-vectors-c07bdb61c9d1). If nodes are similar, but not linked yet, we predict a link between them.\n",
    "\n",
    "The links are predicted on the partial network (all nodes, but 80% of the links are randomly removed) and tested on the ground truth full network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open link dictionaries\n",
    "with open('../preprocessing/preprocessed_data/all_links.json', 'r') as fp:\n",
    "    all_links = json.load(fp)\n",
    "\n",
    "with open('../preprocessing/preprocessed_data/removed_links.json', 'r') as fp:\n",
    "    removed_links = json.load(fp)\n",
    "\n",
    "highest_node_id = int(list(removed_links.keys())[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_links(df):\n",
    "    # Initialize empty predicted links\n",
    "    predicted_links = []\n",
    "\n",
    "    # Iterate over all nodes and compare them to nodes they are not yet linked to\n",
    "    # Predicts link when cosine similarity > 97.5%\n",
    "    for node in range(0, highest_node_id + 1):\n",
    "        # Slices of dataframes containing the node itself\n",
    "        # and the nodes it is not connected to\n",
    "        node_df = df[df.index == node]\n",
    "        unconnected_nodes = [n for n in range(0, highest_node_id + 1) \n",
    "                             if n not in removed_links[str(node)] +\n",
    "                             [node]]\n",
    "        unconnected_nodes_df = df[df.index.isin(unconnected_nodes)]\n",
    "\n",
    "        # Cosine similarity between the node itself\n",
    "        # and the nodes it is not already connected to\n",
    "        similarity_scores = cosine_similarity(node_df, unconnected_nodes_df)[0].tolist()\n",
    "        similarity_dict = dict(zip(unconnected_nodes, similarity_scores))\n",
    "            \n",
    "        # If similarity > 97.5%, append to the list of predictions\n",
    "        for pair in similarity_dict.items():\n",
    "            if pair[1] > 0.975:\n",
    "                predicted_links.append((node, pair[0]))\n",
    "\n",
    "    return predicted_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_dict_to_pairs(link_dict):\n",
    "    pairs = []\n",
    "\n",
    "    # Add a bidirectional pair for each link\n",
    "    # E.g. (1,0) and (0,1)\n",
    "    for node in range(0, highest_node_id + 1):\n",
    "        for neighbor in link_dict[str(node)]:\n",
    "            pairs.append((node, neighbor))\n",
    "            pairs.append((neighbor, node))\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pred):\n",
    "    # Convert the dictionaries to lists of pairs for easier comparison\n",
    "    all_link_pairs = link_dict_to_pairs(all_links)\n",
    "    removed_link_pairs = link_dict_to_pairs(removed_links)\n",
    "    \n",
    "    # False positive predictions\n",
    "    # i.e. it predicted a link that isn't there\n",
    "    fp = set(pred) - set(all_link_pairs)\n",
    "\n",
    "    # False negative predictions\n",
    "    # i.e. it didnt predict a link that should have been there\n",
    "    fn = set(all_link_pairs) - set(pred) - set(removed_link_pairs)\n",
    "\n",
    "    # True positive predictions\n",
    "    # i.e. all correct predictions\n",
    "    tp = set(pred) - fp\n",
    "\n",
    "    # True negative predictions\n",
    "    # i.e. all links that were correctly not predicted\n",
    "    all_possible_pairs = itertools.product(list(range(0, highest_node_id + 1)), list(range(0, highest_node_id + 1)))\n",
    "    tn = set(all_possible_pairs) - set(all_link_pairs) - set(pred)\n",
    "    \n",
    "    total = len(tp) + len(tn) + len(fp) + len(fn)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = (len(tp) + len(tn))/total\n",
    "    \n",
    "    # Precision\n",
    "    precision = len(tp) / (len(tp) + len(fp))\n",
    "    \n",
    "    # Make confusion matrix pairs with (absolute values, percentages)\n",
    "    fp = (len(fp), len(fp)/total)\n",
    "    fn = (len(fn), len(fn)/total)\n",
    "    tp = (len(tp), len(tp)/total)\n",
    "    tn = (len(tn), len(tn)/total)\n",
    "    \n",
    "    return fp, fn, tp, tn, accuracy, precision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node2vec\n",
      " fp:  (4790, 0.000296064590786198) \n",
      " fn:  (32772, 0.002025600995667073) \n",
      " tp:  (9077, 0.000561039309095265) \n",
      " tn:  (16132263, 0.9971172951044515) \n",
      " accuracy:  0.9976783344135467 \n",
      " precision:  0.6545756111631932\n"
     ]
    }
   ],
   "source": [
    "# Import and make predictions\n",
    "node2vec = pd.read_csv('embedded_data/node2vec.csv')\n",
    "node2vec_pred = predict_links(node2vec)\n",
    "\n",
    "fp, fn, tp, tn, accuracy, precision = metrics(node2vec_pred)\n",
    "print('Node2vec\\n',\n",
    "      'fp: ', fp, '\\n',\n",
    "      'fn: ', fn, '\\n',\n",
    "      'tp: ', tp, '\\n',\n",
    "      'tn: ', tn, '\\n',\n",
    "      'accuracy: ', accuracy, '\\n',\n",
    "      'precision: ', precision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
